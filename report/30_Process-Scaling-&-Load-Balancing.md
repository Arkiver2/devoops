## Scaling & Load Balancing

<!-- Applied strategy for scaling and load balancing. -->

Throughout the entire duration of this course, our system has been running by a single server, offered by the cloud provider [DigitalOcean](https://www.digitalocean.com/). We have chosen DigitalOcean since they have a friendly and easy-to-use interface, compared to their competitors, while still providing cheap, stable, and powerful products. To be more precise, we run our entire system on a single "droplet" (DigitalOceans name for a virtual server), with 2 CPU cores and 4 GB of RAM. We figured this was a good balance between cost and compute power to serve the amount of traffic we expected to get. Even if traffic increased, we could always realistically scale the system vertically since DigitalOcean offers much more powerful machines.

Even though the amount of traffic we received could easily be handled by a single server, we still wanted to learn how to scale a system horizontally. Therefore we attempted to transition our system to a distributed docker swarm cluster. However, we were not able to finish this, due to time constraints, so we stuck with our single-machine setup. We managed to get it running locally using three `docker-machine` nodes, but since we had weird timeout issues when connecting to the web application simply stuck with our old setup. Furthermore, we also wanted to provision our new server nodes using terraform, but didn't manage to get started on this. After the project ended we discovered a tool called [Pulumi](https://www.pulumi.com/), which uses javascript to provision our infrastructure. Since we didn't have to learn a new syntax Pulumi might have been easy enough to get started with so that we were able to have learned it in time.

A simpler alternative would have been to simply create another droplet manually through the DigitalOcean UI and provision a load balancer to distribute the traffic between our machines. Besides not having time for it, and not getting to learn how to use docker swarm, this would have also complicated our deployment process greatly since we now need to deploy to two separate machines and make sure no weird race conditions appear. This would also be hard to automate without using an infrastructure automation tool like Terraform or Pulumi.

One of the major blockades for scaling our system was the database. These services are especially hard to scale since they need to be highly persistent, consistent, and performant across machines. If we had the time, there would have been multiple ways of solving this problem. The simplest solution would be to migrating the database to a managed solution similar to the one DigitalOcean provides. This way we could simply scale the capacity or computing power through the UI. This is relatively easy, but could introduce some latency into our system since the application has to contact external devices on every request.
Alternatively, we could try to manage it ourselves. The most obvious solution, but also hardest, would be to set up streaming or replication between Postgres services on each of our machines. While this would ensure that data was available on all nodes in our system, it is cumbersome and hard to set up properly. Especially if the system needs to auto-scaled.
Alternatively, solutions has been build on top of Postgres to provide easier scalability.
TODO: Yugabyte