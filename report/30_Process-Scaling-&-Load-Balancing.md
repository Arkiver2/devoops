## Scaling & Load Balancing

<!-- Applied strategy for scaling and load balancing. -->

Throughout the entire duration of this course, the system has been running by a single server, offered by the cloud provider [DigitalOcean](https://www.digitalocean.com/). We have chosen DigitalOcean since they have a friendly and easy-to-use interface, compared to their competitors, while still providing cheap, stable, and powerful products. To be more precise, we run the entire system on a single "droplet" (DigitalOceans name for a virtual server), with 2 CPU cores and 4 GB of RAM. We figured this was a good balance between cost and compute power to serve the amount of traffic we expected to get. Even if traffic increased, we could always realistically scale the system vertically since DigitalOcean offers much more powerful machines.

Even though the amount of traffic we received could easily be handled by a single server, we still wanted to learn how to scale a system horizontally. That is, being able to scale and distribute a system across many separate machines, with the ability to easily increase or decrease the size of the cluster at will, to accommodate changes in traffic. Therefore we attempted to transition the system to a distributed docker swarm cluster. However, we were not able to finish this, due to time constraints, so we stuck with the single-machine setup. We managed to get it running locally using three `docker-machine` nodes, but since we had weird timeout issues when connecting to the web application simply stuck with the old setup. Furthermore, we also wanted to provision the new server nodes using terraform, but didn't manage to get started on this. After the project ended we discovered a tool called [Pulumi](https://www.pulumi.com/), which uses javascript to provision the infrastructure. Since we didn't have to learn a new syntax Pulumi might have been easy enough to get started with so that we were able to have learned it in time.

A simpler alternative to docker swarm would have been to simply create another droplet manually through the DigitalOcean UI and provision a load balancer to distribute the traffic between the machines. Besides not having time for it, and not learning how to use docker swarm, this would have also complicated the deployment process greatly since we now needed to deploy the services to two separate machines and make sure no weird race conditions appear. This would also be hard to automate without using an infrastructure automation tool like Terraform or Pulumi.

One of the major blockades for scaling the system was the database. These services are especially hard to scale since they need to be highly persistent, consistent, and performant across machines. If we had the time, there would have been multiple ways of solving this problem. The simplest solution would be to migrating the database to a managed solution similar to the one DigitalOcean provides. This way we could simply scale the capacity or computing power through the UI. This is relatively easy, but could introduce some latency into the system since the application has to contact external devices on every request.
Alternatively, we could try to manage it ourselves. The most obvious solution, but also hardest, would be to set up streaming or replication between Postgres services on each of the machines. While this would ensure that data was available on all nodes in the system, it is cumbersome and hard to set up properly. Especially if the system needs to auto-scale in response to increasing demand. This also introduces other edge-cases we needed to handle, like automatic failover if the master node dies, and so on.

Alternatively, existing solutions have been build on top of Postgres to provide easier scalability, such as [Yugabyte](https://www.yugabyte.com/), which we, unfortunately, didn't have time to look into.