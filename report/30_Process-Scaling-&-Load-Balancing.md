## Scaling & Load Balancing

<!-- Applied strategy for scaling and load balancing. -->

Throughout the entire duration of this course, the system has been running by a single server, offered by the cloud provider [DigitalOcean](https://www.digitalocean.com/). DigitalOcean was chosen since they have a friendly and easy-to-use interface, compared to their competitors, while still providing cheap, stable, and powerful products. To be more precise, the entire system has been running on a single "droplet" (DigitalOceans name for a virtual server), with 2 CPU cores and 4 GB of RAM. We figured this was a good balance between cost and compute power to serve the expected amount of traffic. Even if traffic increased, the system could always be scaled vertically since DigitalOcean offers much more powerful machines.

Even though the amount of traffic the system received could easily be handled by a single server, we still wanted to learn how to scale a system horizontally. That is, being able to scale and distribute a system across many separate machines, with the ability to easily increase or decrease the size of the cluster at will, to accommodate changes in traffic. Therefore an attempt was made to transition the system to a distributed docker swarm cluster. However, this was not finished, due to time constraints, so the single-machine setup was kept. We managed to get it running locally using three `docker-machine` nodes, but sometimes weird timeout issues occurred when connecting to the web application. Furthermore, we also wanted to provision the new server nodes using terraform, but didn't manage to get started on this. After the project ended some members of the team discovered a tool called [Pulumi](https://www.pulumi.com/), which uses javascript to provision the infrastructure. Since Pulumi would not have forced us to learn a new syntax, it might have been easy enough to get started with.

A simpler alternative to docker swarm would have been to simply create another droplet manually through the DigitalOcean UI and provision a load balancer to distribute the traffic between the machines. Besides not having time for it, and not learning how to use docker swarm, this would also have complicated the deployment process greatly since service must be deployed to two separate machines, in a way that does not introduce weird race conditions. This would also be hard to automate without using an infrastructure automation tool like Terraform or Pulumi.

One of the major blockades for scaling the system was the database. These services are especially hard to scale since they need to be highly persistent, consistent, and performant across machines. If there was more time left, there would have been multiple ways of solving this problem. The simplest solution would be to migrating the database to a managed solution similar to the one DigitalOcean provides. This way it would have been possible to simply scale the capacity or computing power through the UI. This is relatively easy, but could introduce some latency into the system since the application has to contact external devices on every request.
Alternatively, we could try to manage it ourselves. The most obvious solution, but also hardest, would be to set up streaming or replication between Postgres services on each of the machines. While this would ensure that data was available on all nodes in the system, it is cumbersome and hard to set up properly. Especially if the system needs to auto-scale in response to increasing demand. This also introduces other edge-cases that should be handled, like automatic failover if the master node dies, and so on.

Alternatively, existing solutions have been build on top of Postgres to provide easier scalability, such as [Yugabyte](https://www.yugabyte.com/), which we, unfortunately, didn't have time to look into.