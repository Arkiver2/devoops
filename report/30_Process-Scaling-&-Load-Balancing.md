## Scaling & Load Balancing

<!-- Applied strategy for scaling and load balancing. -->

Throughout the entire duration of this course, the system has been running by a single server, offered by the cloud provider [DigitalOcean](https://www.digitalocean.com/). DigitalOcean was chosen since they have a friendly and easy-to-use interface, compared to their competitors, while still providing cheap, stable, and powerful products. To be more precise, the entire system has been running on a single "droplet" (DigitalOceans name for a virtual server), with 2 CPU cores and 4 GB of RAM. This was chosen as it strikes was a good balance between cost and compute power to serve the expected amount of traffic. Even if traffic increased, the system could always be scaled vertically since DigitalOcean offers much more powerful machines.

Even though the amount of traffic the system received could easily be handled by a single server, learning to scale a system horizontally was still desired. That is, being able to scale and distribute a system across many separate machines, with the ability to easily increase or decrease the size of the cluster at will, to accommodate changes in traffic. Therefore an attempt was made to transition the system to a distributed docker swarm cluster.
However, due to time constraints, the bugs could not be ironed out and the single-machine setup was kept. It was possible to get it running locally using three `docker-machine` nodes, but sometimes weird timeout issues occurred when connecting to the web application. Furthermore, it was the intention to provision the new server nodes using terraform, which again was not possible due to time constraints.

A simpler alternative to docker swarm would have been to simply create another droplet manually through the DigitalOcean UI and provision a load balancer to distribute the traffic between the machines. Besides not having time for it, and not learning how to use docker swarm, this would also have complicated the deployment process greatly since service must be deployed to two separate machines, in a way that does not introduce weird race conditions. This would also be hard to automate without using an infrastructure automation tool like Terraform.

One of the major blockades for scaling the system was the database. These services are especially hard to scale since they need to be highly persistent, consistent, and perform well across multiple machines. If there was more time left, there would have been multiple ways of solving this problem. The simplest solution would be to migrate the database to a managed solution similar to the one DigitalOcean provides. This way it would have been possible to simply scale the capacity or computing power through the UI. This is relatively easy, but could introduce some additional latency into the system since the application has to contact external devices on every request.

Alternatively, the database could self-managed. The most obvious solution, but also hardest, would be to set up streaming or replication between Postgres services on each of the machines. While this would ensure that data was available on all nodes in the system, it is cumbersome and hard to set up properly. Especially if the system needs to auto-scale in response to increasing demand. This also introduces other edge-cases that should be handled, like automatic fail-over if the master node dies, and so on. Since we use a database abstraction layer in our application, we could also investigate if another supported database management system would be more scalable, and relatively easily switch over, assuming the existing data was migrated.
